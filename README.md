# Data_Collection_Project 
An implementation of an industry grade data collection pipeline that runs scalably in the cloud. It uses Python code to automatically control your browser, extract information from a website, and store it on the cloud in a data warehouses and data lake. The system conforms to industry best practices such as being containerised in Docker and running automated tests.

## Milestone 1: Set up the environment
- Source code built using Visual Studio Code.
- GitHub respository created for version control and to track lineage of code over time.
- Virtual environment created and third-party package called 'Selenium' installed. Creating a virtual environment ensures that the project will always run with the version of Selenium that I tested with my code. 
- Selinium is a tool for programmatically controlling a browser.
- Selenium was used with chromedriver: a webdriver used for Google Chrome Version 105.0.5195.125.

## Milestone 2: Selecting website






